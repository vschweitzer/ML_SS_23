{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run generic_pipeline.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from FCLayer import FcLayer\n",
    "from ACLayer import AcLayer\n",
    "from Network import Network\n",
    "from GridSearch import GridSearcher\n",
    "from activation_functions import sigmoid, sigmoid_derivative, tanh, tanh_derivative, relu, relu_derivative\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "epochs = 1000\n",
    "network1 = Network(learning_rate=learning_rate, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.71341844e-02, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.14275511e-02, 2.18253968e-02, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.09811624e-02, 5.95238095e-03, 1.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       ...,\n",
       "       [4.01749844e-04, 2.97619048e-03, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 1.00000000e+00, 0.00000000e+00],\n",
       "       [9.46344076e-03, 9.92063492e-04, 0.00000000e+00, ...,\n",
       "        1.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       ...,\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# input_values = len(X_train.columns)\n",
    "# output_values = 1\n",
    "# intermediate_values = 3\n",
    "# layers = 3\n",
    "\n",
    "# for j in range(layers):\n",
    "#     i = intermediate_values\n",
    "#     o = intermediate_values\n",
    "\n",
    "#     if not j:\n",
    "#         i = input_values\n",
    "#     elif j + 1 == layers:\n",
    "#         o = output_values\n",
    "\n",
    "#     fc = FcLayer(i, o)\n",
    "#     ac = AcLayer(tanh, tanh_prime)\n",
    "#     network1.add(fc)\n",
    "#     network1.add(ac)\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1  2  3  4  5  6  7\n",
       "69   0  0  0  0  0  0  1  0\n",
       "190  0  0  0  0  1  0  0  0\n",
       "73   0  0  0  0  0  1  0  0\n",
       "162  0  0  0  0  0  1  0  0\n",
       "186  0  1  0  0  0  0  0  0\n",
       "..  .. .. .. .. .. .. .. ..\n",
       "83   0  0  0  0  0  1  0  0\n",
       "39   0  0  0  0  0  1  0  0\n",
       "140  0  1  0  0  0  0  0  0\n",
       "70   0  0  0  0  0  0  1  0\n",
       "131  0  0  0  0  0  1  0  0\n",
       "\n",
       "[155 rows x 8 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#network1.train(np.expand_dims(X_train.to_numpy(), axis=1), np.expand_dims(y_train.to_numpy(), axis=1), epochs=epochs)\n",
    "network1.fit(X_train, y_train)\n",
    "y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(X_train.to_numpy(), axis=1).shape[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 1, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 1, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 1, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "out = network1.predict(X_train)\n",
    "out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1  2  3  4  5  6  7\n",
       "69   0  0  0  0  0  0  1  0\n",
       "190  0  0  0  0  1  0  0  0\n",
       "73   0  0  0  0  0  1  0  0\n",
       "162  0  0  0  0  0  1  0  0\n",
       "186  0  1  0  0  0  0  0  0\n",
       "..  .. .. .. .. .. .. .. ..\n",
       "83   0  0  0  0  0  1  0  0\n",
       "39   0  0  0  0  0  1  0  0\n",
       "140  0  1  0  0  0  0  0  0\n",
       "70   0  0  0  0  0  0  1  0\n",
       "131  0  0  0  0  0  1  0  0\n",
       "\n",
       "[155 rows x 8 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 65)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 432: 0.05350787053952015\n",
      "2 / 432: 0.07146696335273921\n",
      "3 / 432: 0.02905353661932609\n",
      "4 / 432: 0.1104498494800219\n",
      "5 / 432: 0.022913302913302912\n",
      "6 / 432: 0.027573260073260074\n",
      "7 / 432: 0.06506950350990598\n",
      "8 / 432: 0.012266312316715542\n",
      "9 / 432: 0.06946932090301916\n",
      "10 / 432: 0.011138411138411138\n",
      "11 / 432: 0.010567067736185385\n",
      "12 / 432: 0.010567067736185385\n",
      "13 / 432: 0.010567067736185385\n",
      "14 / 432: 0.010567067736185385\n",
      "15 / 432: 0.010567067736185385\n",
      "16 / 432: 0.0638040176198071\n",
      "17 / 432: 0.01753319597069597\n",
      "18 / 432: 0.028703703703703703\n",
      "19 / 432: 0.045963545226703116\n",
      "20 / 432: 0.015995816430020286\n",
      "21 / 432: 0.04246583756282032\n",
      "22 / 432: 0.07001736838430386\n",
      "23 / 432: 0.010567067736185385\n",
      "24 / 432: 0.05673904400469865\n",
      "25 / 432: 0.03820174652933273\n",
      "26 / 432: 0.010567067736185385\n",
      "27 / 432: 0.01731060606060606\n",
      "28 / 432: 0.060972222222222226\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m parameters \u001b[39m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m0.01\u001b[39m, \u001b[39m0.1\u001b[39m,\u001b[39m0.5\u001b[39m,\u001b[39m0.8\u001b[39m],\n\u001b[0;32m      3\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m10\u001b[39m,\u001b[39m100\u001b[39m,\u001b[39m1000\u001b[39m,\u001b[39m2000\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m }\n\u001b[0;32m      8\u001b[0m gs \u001b[39m=\u001b[39m GridSearcher(network1, parameters, pd\u001b[39m.\u001b[39mDataFrame(X_train), pd\u001b[39m.\u001b[39mDataFrame(y_train))\n\u001b[1;32m----> 9\u001b[0m gs_results \u001b[39m=\u001b[39m gs\u001b[39m.\u001b[39;49msearch()\n\u001b[0;32m     10\u001b[0m gs_results\n",
      "File \u001b[1;32mc:\\Users\\vali\\Documents\\GitHub\\Studium\\ml\\ML_SS_23\\src\\GridSearch.py:73\u001b[0m, in \u001b[0;36mGridSearcher.search\u001b[1;34m(self, folds, n_jobs, verbose, scoring)\u001b[0m\n\u001b[0;32m     71\u001b[0m fold_scores \u001b[39m=\u001b[39m []\n\u001b[0;32m     72\u001b[0m fit_times \u001b[39m=\u001b[39m []\n\u001b[1;32m---> 73\u001b[0m predict_times \u001b[39m=\u001b[39m []\n\u001b[0;32m     74\u001b[0m \u001b[39mfor\u001b[39;00m i, (train_index, test_index) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(kf\u001b[39m.\u001b[39msplit(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx)):\n\u001b[0;32m     75\u001b[0m     X_train \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx\u001b[39m.\u001b[39miloc[train_index]\n",
      "File \u001b[1;32mc:\\Users\\vali\\Documents\\GitHub\\Studium\\ml\\ML_SS_23\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    265\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 266\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    268\u001b[0m         clone(estimator),\n\u001b[0;32m    269\u001b[0m         X,\n\u001b[0;32m    270\u001b[0m         y,\n\u001b[0;32m    271\u001b[0m         scorers,\n\u001b[0;32m    272\u001b[0m         train,\n\u001b[0;32m    273\u001b[0m         test,\n\u001b[0;32m    274\u001b[0m         verbose,\n\u001b[0;32m    275\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    276\u001b[0m         fit_params,\n\u001b[0;32m    277\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[0;32m    278\u001b[0m         return_times\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    279\u001b[0m         return_estimator\u001b[39m=\u001b[39;49mreturn_estimator,\n\u001b[0;32m    280\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[0;32m    281\u001b[0m     )\n\u001b[0;32m    282\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m cv\u001b[39m.\u001b[39;49msplit(X, y, groups)\n\u001b[0;32m    283\u001b[0m )\n\u001b[0;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    287\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vali\\Documents\\GitHub\\Studium\\ml\\ML_SS_23\\venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\vali\\Documents\\GitHub\\Studium\\ml\\ML_SS_23\\venv\\Lib\\site-packages\\joblib\\parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1098\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[0;32m   1099\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[1;32mc:\\Users\\vali\\Documents\\GitHub\\Studium\\ml\\ML_SS_23\\venv\\Lib\\site-packages\\joblib\\parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    974\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m--> 975\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[0;32m    976\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[1;32mc:\\Users\\vali\\Documents\\GitHub\\Studium\\ml\\ML_SS_23\\venv\\Lib\\site-packages\\joblib\\_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 567\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[0;32m    568\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    569\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m    449\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[1;32m--> 451\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[0;32m    454\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[0;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'learning_rate': [0.5,0.8],\n",
    "    'epochs': [1000,2000],\n",
    "    'node_counts': [[3,3],[5,5],[10,10],[50,50],[100,100],[3,3,3],[5,5,5],[10,10,10],[20,20,20]],\n",
    "    'activation_function': [(sigmoid,sigmoid_derivative),(relu,relu_derivative),(tanh,tanh_derivative)],\n",
    "\n",
    "}\n",
    "gs = GridSearcher(network1, parameters, pd.DataFrame(X_train), pd.DataFrame(y_train))\n",
    "gs_results = gs.search()\n",
    "gs_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.19618056, 0.07407407, 0.04807692, 0.08504539, 0.04464286])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_results[\"test_score\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08960396, 0.04499002, 0.01452273, 0.07522672, 0.03616415,\n",
       "       0.02337381, 0.07130482, 0.01951762, 0.06039978, 0.01817755,\n",
       "       0.00883929, 0.05613495, 0.01382239, 0.03662442, 0.05446491,\n",
       "       0.0205635 , 0.03809287, 0.06789895, 0.06909277, 0.02720154,\n",
       "       0.01692833, 0.05539217, 0.04597683, 0.0248027 , 0.0404798 ,\n",
       "       0.02381525, 0.06039317, 0.05903014, 0.02457636, 0.0419113 ,\n",
       "       0.04023377, 0.0871844 , 0.05557678, 0.09452476, 0.06286417,\n",
       "       0.02191144, 0.04889153, 0.04195395, 0.06753563, 0.02103175,\n",
       "       0.02769222, 0.07343637, 0.01470814, 0.06265678, 0.06210229,\n",
       "       0.04687996, 0.06987879, 0.08929224, 0.03752392, 0.01888113,\n",
       "       0.06345015, 0.02032585, 0.05464656, 0.06524574, 0.0045928 ,\n",
       "       0.02568841, 0.06414377, 0.05427285, 0.05514984, 0.03612816,\n",
       "       0.06969607, 0.08247475, 0.05216267, 0.08314587, 0.02644494,\n",
       "       0.05235752, 0.07771837, 0.01518479, 0.09112311, 0.03127212,\n",
       "       0.06451915, 0.0158428 , 0.03716054, 0.09322437, 0.        ,\n",
       "       0.02839496, 0.07173714, 0.04554179, 0.0242747 , 0.02885588,\n",
       "       0.06636029, 0.00918561, 0.05179174, 0.09378912, 0.02143162,\n",
       "       0.06579489, 0.04527393, 0.04080946, 0.06114719, 0.01325105,\n",
       "       0.02602425, 0.06860341, 0.0300204 , 0.0546021 , 0.03508421,\n",
       "       0.0362676 , 0.00874459, 0.07011201, 0.03001955, 0.01896212,\n",
       "       0.06478003, 0.01661505, 0.05550473, 0.07339197, 0.04646494,\n",
       "       0.01842105, 0.0931555 , 0.04158171, 0.02861162, 0.06118534,\n",
       "       0.04545004, 0.07696841, 0.04452574, 0.04743546, 0.03393704,\n",
       "       0.06156037, 0.03814532, 0.04546181, 0.07230827, 0.02547123,\n",
       "       0.03319009, 0.07502021, 0.03947836, 0.03734921, 0.06683339,\n",
       "       0.00753676, 0.01853355, 0.0755985 , 0.04785291, 0.06749858,\n",
       "       0.04048556, 0.07165906, 0.0077178 , 0.06531518, 0.0217304 ,\n",
       "       0.09730463, 0.0884104 , 0.02131944, 0.04578755, 0.04224454,\n",
       "       0.05177671, 0.02010879, 0.09081657, 0.01077303, 0.04533998,\n",
       "       0.04932423, 0.02822099, 0.05135351, 0.04967073, 0.07066799,\n",
       "       0.03159894, 0.08302781, 0.03056234, 0.11202861, 0.03425969,\n",
       "       0.09627671, 0.02636541, 0.02606322, 0.07172839, 0.04371413,\n",
       "       0.0846752 , 0.02358974, 0.06279304, 0.07606977, 0.03347418,\n",
       "       0.08257867, 0.00727679, 0.05374734, 0.11650153, 0.06248356,\n",
       "       0.05741002, 0.00892857, 0.02691763, 0.07672928, 0.02425445,\n",
       "       0.05830317, 0.0285542 , 0.06056284, 0.04619872, 0.07556687,\n",
       "       0.02188835, 0.06491832, 0.05407028, 0.19979324, 0.09457149,\n",
       "       0.12284946, 0.03036603, 0.02716564, 0.04082967, 0.05713839,\n",
       "       0.04109045, 0.03672466, 0.06729698, 0.05280616, 0.0732313 ,\n",
       "       0.02961452, 0.19865654, 0.04074074, 0.24732797, 0.09437629,\n",
       "       0.34066384, 0.04547933, 0.0184152 , 0.06057256, 0.03936581,\n",
       "       0.04897405, 0.05171382, 0.08508331, 0.04856968, 0.04227149,\n",
       "       0.03066188, 0.09032389, 0.02688099, 0.08483618, 0.01351051,\n",
       "       0.0577143 , 0.04853731, 0.02232934, 0.07376743, 0.04949265,\n",
       "       0.09565348, 0.01604167, 0.08641781, 0.03743192, 0.05742641,\n",
       "       0.02186926, 0.09730298, 0.07679866, 0.09323505, 0.02979799,\n",
       "       0.08119662, 0.02543088, 0.01568903, 0.0540801 , 0.05765396,\n",
       "       0.05203814, 0.03541366, 0.05348582, 0.05036376, 0.06012353,\n",
       "       0.02089908, 0.17293185, 0.03406545, 0.28204939, 0.08878136,\n",
       "       0.21538082, 0.03830366, 0.02221558, 0.05121988, 0.07267868,\n",
       "       0.05770041, 0.04917559, 0.05078719, 0.0597864 , 0.07431203,\n",
       "       0.02044363, 0.24252891, 0.03067065, 0.32583305, 0.04694112,\n",
       "       0.28901685, 0.09306277, 0.03128323, 0.05165307, 0.014375  ,\n",
       "       0.05407951, 0.05951868, 0.03206773, 0.06471306, 0.03633794,\n",
       "       0.07228095, 0.0174053 ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_results[\"score_mean\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_sklearn = MLPClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_sk = {\n",
    "    'learning_rate_init': [0.01, 0.1,0.5,0.8],\n",
    "    \"learning_rate\": [\"constant\"],\n",
    "    'max_iter': [10,100,1000,2000],\n",
    "    'hidden_layer_sizes': [[3,3],[5,5],[10,10],[50,50],[100,100],[3,3,3],[5,5,5],[10,10,10],[20,20,20]],\n",
    "    'activation': [\"logistic\", \"relu\", \"tanh\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_nn = MLPClassifier()\n",
    "\n",
    "gs_sk = GridSearcher(sk_nn, parameters_sk, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 432: 0.014263411138411137\n",
      "2 / 432: 0.1046290963342017\n",
      "3 / 432: 0.06972707586618877\n",
      "4 / 432: 0.05651296072348703\n",
      "5 / 432: 0.07472637055497763\n",
      "6 / 432: 0.043243959325870426\n",
      "7 / 432: 0.0\n",
      "8 / 432: 0.019355292792792793\n",
      "9 / 432: 0.01666666666666667\n",
      "10 / 432: 0.0\n",
      "11 / 432: 0.1272774739880003\n",
      "12 / 432: 0.19470598845598847\n",
      "13 / 432: 0.0\n",
      "14 / 432: 0.2964404270480122\n",
      "15 / 432: 0.32865897990897985\n",
      "16 / 432: 0.04648779311608259\n",
      "17 / 432: 0.06931174350388747\n",
      "18 / 432: 0.05584842010934965\n",
      "19 / 432: 0.023379240484503645\n",
      "20 / 432: 0.04521747076023393\n",
      "21 / 432: 0.12969391784619488\n",
      "22 / 432: 0.009210526315789473\n",
      "23 / 432: 0.01902126099706745\n",
      "24 / 432: 0.009210526315789473\n",
      "25 / 432: 0.0\n",
      "26 / 432: 0.02777777777777778\n",
      "27 / 432: 0.0\n",
      "28 / 432: 0.0\n",
      "29 / 432: 0.026785714285714284\n",
      "30 / 432: 0.01111111111111111\n",
      "31 / 432: 0.0\n",
      "32 / 432: 0.16230583426235595\n",
      "33 / 432: 0.14120549771865565\n",
      "34 / 432: 0.0\n",
      "35 / 432: 0.3774545169176008\n",
      "36 / 432: 0.2948984933195459\n",
      "37 / 432: 0.3019852661957925\n",
      "38 / 432: 0.384169478182636\n",
      "39 / 432: 0.38529797077707295\n",
      "40 / 432: 0.32983601194127504\n",
      "41 / 432: 0.39356224125405714\n",
      "42 / 432: 0.402484675412307\n",
      "43 / 432: 0.0\n",
      "44 / 432: 0.02904761904761905\n",
      "45 / 432: 0.033392857142857134\n",
      "46 / 432: 0.0\n",
      "47 / 432: 0.184158998895841\n",
      "48 / 432: 0.07237394957983193\n",
      "49 / 432: 0.0\n",
      "50 / 432: 0.36276022413894266\n",
      "51 / 432: 0.2804894028549635\n",
      "52 / 432: 0.01875\n",
      "53 / 432: 0.3756883231147937\n",
      "54 / 432: 0.3778857253857254\n",
      "55 / 432: 0.12148167621851833\n",
      "56 / 432: 0.2507408673198147\n",
      "57 / 432: 0.08793015146562058\n",
      "58 / 432: 0.33342983384005054\n",
      "59 / 432: 0.3339696565511783\n",
      "60 / 432: 0.3157333026450673\n",
      "61 / 432: 0.36782656861318186\n",
      "62 / 432: 0.38039502529634106\n",
      "63 / 432: 0.38825507285747557\n",
      "64 / 432: 0.37152231953147286\n",
      "65 / 432: 0.39789337537621294\n",
      "66 / 432: 0.3854633297988561\n",
      "67 / 432: 0.2348295062425497\n",
      "68 / 432: 0.39826287274816685\n",
      "69 / 432: 0.38168625964678593\n",
      "70 / 432: 0.037450396825396824\n",
      "71 / 432: 0.20814877400235457\n",
      "72 / 432: 0.1103373015873016\n",
      "73 / 432: 0.049229691876750706\n",
      "74 / 432: 0.23657734078786713\n",
      "75 / 432: 0.3177370400067769\n",
      "76 / 432: 0.30860720978368034\n",
      "77 / 432: 0.3590772780430709\n",
      "78 / 432: 0.33538575897786427\n",
      "79 / 432: 0.37259100376747434\n",
      "80 / 432: 0.3707099900163401\n",
      "81 / 432: 0.3710046354611572\n",
      "82 / 432: 0.24151744862464675\n",
      "83 / 432: 0.28721752104105047\n",
      "84 / 432: 0.2948548690227079\n",
      "85 / 432: 0.33403388686476926\n",
      "86 / 432: 0.30316645275724563\n",
      "87 / 432: 0.29906372914525087\n",
      "88 / 432: 0.34655826820014457\n",
      "89 / 432: 0.3669698357198357\n",
      "90 / 432: 0.39473906468488507\n",
      "91 / 432: 0.366062017094794\n",
      "92 / 432: 0.3897892630245572\n",
      "93 / 432: 0.38956873841187006\n",
      "94 / 432: 0.2820596305924076\n",
      "95 / 432: 0.40060030612662195\n",
      "96 / 432: 0.40419098385274854\n",
      "97 / 432: 0.03404558404558404\n",
      "98 / 432: 0.20220039828592462\n",
      "99 / 432: 0.1656170369485587\n",
      "100 / 432: 0.1861275639565113\n",
      "101 / 432: 0.23326839826839824\n",
      "102 / 432: 0.3310432372102852\n",
      "103 / 432: 0.19737930654171063\n",
      "104 / 432: 0.327953216374269\n",
      "105 / 432: 0.3439434361461215\n",
      "106 / 432: 0.27559846972772667\n",
      "107 / 432: 0.3440460256118151\n",
      "108 / 432: 0.389906331374463\n",
      "109 / 432: 0.0\n",
      "110 / 432: 0.015384615384615382\n",
      "111 / 432: 0.0\n",
      "112 / 432: 0.0\n",
      "113 / 432: 0.05799159356725146\n",
      "114 / 432: 0.00625\n",
      "115 / 432: 0.0\n",
      "116 / 432: 0.1505011655011655\n",
      "117 / 432: 0.056994047619047604\n",
      "118 / 432: 0.0\n",
      "119 / 432: 0.2582846157110863\n",
      "120 / 432: 0.18268391330891331\n",
      "121 / 432: 0.009210526315789473\n",
      "122 / 432: 0.07824055330634279\n",
      "123 / 432: 0.07921052631578948\n",
      "124 / 432: 0.0\n",
      "125 / 432: 0.023611111111111117\n",
      "126 / 432: 0.015\n",
      "127 / 432: 0.0\n",
      "128 / 432: 0.060352869352869355\n",
      "129 / 432: 0.0\n",
      "130 / 432: 0.0\n",
      "131 / 432: 0.045\n",
      "132 / 432: 0.06515873015873015\n",
      "133 / 432: 0.0\n",
      "134 / 432: 0.11425367808519982\n",
      "135 / 432: 0.08764061421670118\n",
      "136 / 432: 0.09352066062592379\n",
      "137 / 432: 0.09874999999999999\n",
      "138 / 432: 0.12247668997668996\n",
      "139 / 432: 0.11258430166324902\n",
      "140 / 432: 0.21611390071916387\n",
      "141 / 432: 0.2932561227192066\n",
      "142 / 432: 0.29606227106227107\n",
      "143 / 432: 0.33229616542550244\n",
      "144 / 432: 0.355320472655999\n",
      "145 / 432: 0.2457752477141022\n",
      "146 / 432: 0.38131841112104264\n",
      "147 / 432: 0.3799085673098831\n",
      "148 / 432: 0.0\n",
      "149 / 432: 0.28737697373146287\n",
      "150 / 432: 0.09315126050420167\n",
      "151 / 432: 0.04221014492753623\n",
      "152 / 432: 0.09075966183574878\n",
      "153 / 432: 0.09444871794871795\n",
      "154 / 432: 0.01\n",
      "155 / 432: 0.18555716213610948\n",
      "156 / 432: 0.12830759394058627\n",
      "157 / 432: 0.09803030303030302\n",
      "158 / 432: 0.3002900391699356\n",
      "159 / 432: 0.265398073195556\n",
      "160 / 432: 0.06087962962962964\n",
      "161 / 432: 0.37483104906789116\n",
      "162 / 432: 0.2495292207792208\n",
      "163 / 432: 0.23706796591781112\n",
      "164 / 432: 0.2509373813321182\n",
      "165 / 432: 0.08964285714285714\n",
      "166 / 432: 0.3008460191376802\n",
      "167 / 432: 0.2485928805494023\n",
      "168 / 432: 0.29474871444980144\n",
      "169 / 432: 0.37593739593739595\n",
      "170 / 432: 0.297709373959374\n",
      "171 / 432: 0.35037569714427297\n",
      "172 / 432: 0.28436752136752136\n",
      "173 / 432: 0.3504635135713511\n",
      "174 / 432: 0.3782319373952501\n",
      "175 / 432: 0.22209574242182936\n",
      "176 / 432: 0.2841540404040404\n",
      "177 / 432: 0.05373316199929103\n",
      "178 / 432: 0.12656920564815302\n",
      "179 / 432: 0.08857142857142858\n",
      "180 / 432: 0.06761904761904762\n",
      "181 / 432: 0.2570682095682096\n",
      "182 / 432: 0.2527553988409251\n",
      "183 / 432: 0.18010475138672072\n",
      "184 / 432: 0.16751379296657934\n",
      "185 / 432: 0.367449864949865\n",
      "186 / 432: 0.19956275213434654\n",
      "187 / 432: 0.0\n",
      "188 / 432: 0.31865026881331227\n",
      "189 / 432: 0.3486846748023218\n",
      "190 / 432: 0.13941813668787356\n",
      "191 / 432: 0.07244071717755927\n",
      "192 / 432: 0.15220045069309773\n",
      "193 / 432: 0.31766554395037366\n",
      "194 / 432: 0.3216082657452627\n",
      "195 / 432: 0.27835018438733605\n",
      "196 / 432: 0.3268483490194017\n",
      "197 / 432: 0.36334495037126613\n",
      "198 / 432: 0.3523924184298225\n",
      "199 / 432: 0.3769209080393291\n",
      "200 / 432: 0.33584595959595964\n",
      "201 / 432: 0.34988829153302836\n",
      "202 / 432: 0.0\n",
      "203 / 432: 0.21945699888346945\n",
      "204 / 432: 0.19623002601263473\n",
      "205 / 432: 0.13652946515849743\n",
      "206 / 432: 0.05009090909090909\n",
      "207 / 432: 0.11397631655783828\n",
      "208 / 432: 0.11005638279345176\n",
      "209 / 432: 0.24013017684070315\n",
      "210 / 432: 0.2544755742772804\n",
      "211 / 432: 0.2725548793311951\n",
      "212 / 432: 0.26430429219902907\n",
      "213 / 432: 0.3274339111181216\n",
      "214 / 432: 0.042499999999999996\n",
      "215 / 432: 0.35287848408901035\n",
      "216 / 432: 0.2572383498854087\n",
      "217 / 432: 0.0\n",
      "218 / 432: 0.0\n",
      "219 / 432: 0.007692307692307693\n",
      "220 / 432: 0.0\n",
      "221 / 432: 0.0125\n",
      "222 / 432: 0.026644736842105266\n",
      "223 / 432: 0.0\n",
      "224 / 432: 0.0041666666666666675\n",
      "225 / 432: 0.019872813990461046\n",
      "226 / 432: 0.047178055432699394\n",
      "227 / 432: 0.0\n",
      "228 / 432: 0.04523355263157895\n",
      "229 / 432: 0.01731863442389758\n",
      "230 / 432: 0.00625\n",
      "231 / 432: 0.02413533834586466\n",
      "232 / 432: 0.0\n",
      "233 / 432: 0.0\n",
      "234 / 432: 0.0\n",
      "235 / 432: 0.0\n",
      "236 / 432: 0.0\n",
      "237 / 432: 0.01944444444444444\n",
      "238 / 432: 0.0\n",
      "239 / 432: 0.013953488372093023\n",
      "240 / 432: 0.023164014687882494\n",
      "241 / 432: 0.0\n",
      "242 / 432: 0.0\n",
      "243 / 432: 0.030059523809523814\n",
      "244 / 432: 0.037884615384615385\n",
      "245 / 432: 0.015\n",
      "246 / 432: 0.007142857142857143\n",
      "247 / 432: 0.057751710654936464\n",
      "248 / 432: 0.04652777777777777\n",
      "249 / 432: 0.028869047619047617\n",
      "250 / 432: 0.05089322862492198\n",
      "251 / 432: 0.0\n",
      "252 / 432: 0.03\n",
      "253 / 432: 0.030716936572199725\n",
      "254 / 432: 0.007142857142857143\n",
      "255 / 432: 0.03962439883492515\n",
      "256 / 432: 0.019358108108108108\n",
      "257 / 432: 0.01\n",
      "258 / 432: 0.055552761408024565\n",
      "259 / 432: 0.0\n",
      "260 / 432: 0.01875\n",
      "261 / 432: 0.0\n",
      "262 / 432: 0.014285714285714285\n",
      "263 / 432: 0.0\n",
      "264 / 432: 0.0071428571428571435\n",
      "265 / 432: 0.0\n",
      "266 / 432: 0.0\n",
      "267 / 432: 0.009210526315789473\n",
      "268 / 432: 0.0\n",
      "269 / 432: 0.0\n",
      "270 / 432: 0.009210526315789473\n",
      "271 / 432: 0.0\n",
      "272 / 432: 0.04056390977443609\n",
      "273 / 432: 0.030833333333333334\n",
      "274 / 432: 0.048006379585326955\n",
      "275 / 432: 0.051637700534759357\n",
      "276 / 432: 0.0429951690821256\n",
      "277 / 432: 0.016666666666666666\n",
      "278 / 432: 0.0\n",
      "279 / 432: 0.045687012263099215\n",
      "280 / 432: 0.019358108108108108\n",
      "281 / 432: 0.0\n",
      "282 / 432: 0.05243689883492515\n",
      "283 / 432: 0.05453634395494861\n",
      "284 / 432: 0.00625\n",
      "285 / 432: 0.02617481203007519\n",
      "286 / 432: 0.05565934065934066\n",
      "287 / 432: 0.0\n",
      "288 / 432: 0.0\n",
      "289 / 432: 0.03035714285714286\n",
      "290 / 432: 0.0\n",
      "291 / 432: 0.017391304347826087\n",
      "292 / 432: 0.0\n",
      "293 / 432: 0.0\n",
      "294 / 432: 0.01818181818181818\n",
      "295 / 432: 0.0\n",
      "296 / 432: 0.0\n",
      "297 / 432: 0.0\n",
      "298 / 432: 0.0\n",
      "299 / 432: 0.0\n",
      "300 / 432: 0.004999999999999999\n",
      "301 / 432: 0.04466403162055336\n",
      "302 / 432: 0.04991258741258741\n",
      "303 / 432: 0.06664919711360201\n",
      "304 / 432: 0.0\n",
      "305 / 432: 0.03613636363636363\n",
      "306 / 432: 0.009999999999999998\n",
      "307 / 432: 0.018421052631578946\n",
      "308 / 432: 0.0\n",
      "309 / 432: 0.0808500733152436\n",
      "310 / 432: 0.0\n",
      "311 / 432: 0.017142857142857144\n",
      "312 / 432: 0.02173039912977993\n",
      "313 / 432: 0.0\n",
      "314 / 432: 0.0\n",
      "315 / 432: 0.0\n",
      "316 / 432: 0.026778656126482215\n",
      "317 / 432: 0.03871212121212121\n",
      "318 / 432: 0.014583333333333332\n",
      "319 / 432: 0.015217391304347827\n",
      "320 / 432: 0.0125\n",
      "321 / 432: 0.008108108108108107\n",
      "322 / 432: 0.0\n",
      "323 / 432: 0.0\n",
      "324 / 432: 0.0\n",
      "325 / 432: 0.0\n",
      "326 / 432: 0.018421052631578946\n",
      "327 / 432: 0.020938215102974826\n",
      "328 / 432: 0.0\n",
      "329 / 432: 0.0\n",
      "330 / 432: 0.01111111111111111\n",
      "331 / 432: 0.0\n",
      "332 / 432: 0.0\n",
      "333 / 432: 0.0380676222781486\n",
      "334 / 432: 0.03280756481220878\n",
      "335 / 432: 0.0\n",
      "336 / 432: 0.03231800673661138\n",
      "337 / 432: 0.02230576441102757\n",
      "338 / 432: 0.023056680161943323\n",
      "339 / 432: 0.02349300193050193\n",
      "340 / 432: 0.0\n",
      "341 / 432: 0.0\n",
      "342 / 432: 0.009210526315789473\n",
      "343 / 432: 0.0\n",
      "344 / 432: 0.0\n",
      "345 / 432: 0.020460526315789474\n",
      "346 / 432: 0.0\n",
      "347 / 432: 0.013095238095238096\n",
      "348 / 432: 0.021451355661881978\n",
      "349 / 432: 0.03151629072681704\n",
      "350 / 432: 0.0\n",
      "351 / 432: 0.017200854700854702\n",
      "352 / 432: 0.016666666666666666\n",
      "353 / 432: 0.0\n",
      "354 / 432: 0.0\n",
      "355 / 432: 0.01666666666666667\n",
      "356 / 432: 0.0\n",
      "357 / 432: 0.0\n",
      "358 / 432: 0.0\n",
      "359 / 432: 0.0125\n",
      "360 / 432: 0.0125\n",
      "361 / 432: 0.06360809363525091\n",
      "362 / 432: 0.0\n",
      "363 / 432: 0.06975425923865336\n",
      "364 / 432: 0.0344140146878825\n",
      "365 / 432: 0.0\n",
      "366 / 432: 0.05359928847841945\n",
      "367 / 432: 0.0\n",
      "368 / 432: 0.0\n",
      "369 / 432: 0.0\n",
      "370 / 432: 0.0\n",
      "371 / 432: 0.011111111111111112\n",
      "372 / 432: 0.0\n",
      "373 / 432: 0.0\n",
      "374 / 432: 0.0\n",
      "375 / 432: 0.009210526315789473\n",
      "376 / 432: 0.009210526315789473\n",
      "377 / 432: 0.009210526315789473\n",
      "378 / 432: 0.017200854700854702\n",
      "379 / 432: 0.0\n",
      "380 / 432: 0.0\n",
      "381 / 432: 0.0\n",
      "382 / 432: 0.020833333333333336\n",
      "383 / 432: 0.0\n",
      "384 / 432: 0.03861111111111111\n",
      "385 / 432: 0.010714285714285714\n",
      "386 / 432: 0.0\n",
      "387 / 432: 0.05392857142857144\n",
      "388 / 432: 0.026717529116909923\n",
      "389 / 432: 0.0125\n",
      "390 / 432: 0.04381217466743783\n",
      "391 / 432: 0.052381099281321584\n",
      "392 / 432: 0.0\n",
      "393 / 432: 0.02967105263157895\n",
      "394 / 432: 0.0\n",
      "395 / 432: 0.0\n",
      "396 / 432: 0.0\n",
      "397 / 432: 0.0\n",
      "398 / 432: 0.0\n",
      "399 / 432: 0.00625\n",
      "400 / 432: 0.0\n",
      "401 / 432: 0.0\n",
      "402 / 432: 0.01346153846153846\n",
      "403 / 432: 0.0\n",
      "404 / 432: 0.0\n",
      "405 / 432: 0.0\n",
      "406 / 432: 0.0\n",
      "407 / 432: 0.0\n",
      "408 / 432: 0.013333333333333332\n",
      "409 / 432: 0.03214285714285715\n",
      "410 / 432: 0.0\n",
      "411 / 432: 0.027922077922077924\n",
      "412 / 432: 0.0\n",
      "413 / 432: 0.0\n",
      "414 / 432: 0.009210526315789473\n",
      "415 / 432: 0.02757577939376485\n",
      "416 / 432: 0.01388888888888889\n",
      "417 / 432: 0.024345238095238097\n",
      "418 / 432: 0.03888157894736842\n",
      "419 / 432: 0.0\n",
      "420 / 432: 0.044328790726817044\n",
      "421 / 432: 0.0\n",
      "422 / 432: 0.0\n",
      "423 / 432: 0.0\n",
      "424 / 432: 0.016129032258064516\n",
      "425 / 432: 0.0\n",
      "426 / 432: 0.0\n",
      "427 / 432: 0.0\n",
      "428 / 432: 0.0\n",
      "429 / 432: 0.0\n",
      "430 / 432: 0.0\n",
      "431 / 432: 0.013095238095238096\n",
      "432 / 432: 0.024078804078804077\n"
     ]
    }
   ],
   "source": [
    "gs_results_sk = gs_sk.search()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37c7361227dc3e7733ba0d9221653a542190e1fc763a7f303852558240223e6a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
